{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Semana 2\n",
    "- Exercícios do laboratório da segunda semana do curso de machine learning da DeepLearning.AI em parceira com a Universidade de Stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1:\n",
    "- Traduzir para código a função do custo para um modelo de regressão linear\n",
    "- A função de custo é dada por: \n",
    "    $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_custo(x, y, w, b): \n",
    "    \"\"\"\n",
    "   calcula a função de custo para modelos de regressão linear\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray): Shape (m,) dados do modelo a ser treinado\n",
    "        y (ndarray): Shape (m,) caracteristicas dos dados do modelo\n",
    "        w, b : Parametros do modelo\n",
    "    \n",
    "    Returns\n",
    "        custo_total (float): O custo de usar w,b como parâmetros para regressão\n",
    "          linear para os dados nos pontos x,y  \n",
    "    \"\"\"\n",
    "\n",
    "    # número de exemplos de treinamento\n",
    "    m = x.shape[0] \n",
    "    \n",
    "    soma_custo = 0\n",
    "\n",
    "    # variável a ser retornada\n",
    "    custo_total = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        custo = (f_wb - y[i]) ** 2\n",
    "        soma_custo += custo \n",
    "    custo_total =(1 / (2*m)) * soma_custo    \n",
    "\n",
    "    return custo_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2\n",
    "- Traduzir para código uma função que calcule a descida do gradiente dos parâmetros w,b para regressão linear\n",
    "- A descida do gradiente é dada por:\n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)} \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_gradiente(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray): Shape (m,) dados do modelo a ser treinado\n",
    "      y (ndarray): Shape (m,) caracteristicas dos dados do modelo\n",
    "      w, b: Parametros do modelo\n",
    "    Returns\n",
    "      dj_dw : O gradiente do custo w.r.t. para os parâmetros w\n",
    "      dj_db : O gradiente do custo w.r.t. para os parâmetros b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Números de exemplos de treinmaneto\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    # variáveis a serem retornadas\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i]\n",
    "        dj_db_i = f_wb -y[i]\n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i\n",
    "\n",
    "    dj_dw = dj_dw/m\n",
    "    dj_db = dj_db/m\n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
